{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = pd.read_csv('/Users/shailendrapatil/Spring 2017/ML/Assignments/Extra Credit Programming/traning_data.csv', delimiter=',')\n",
    "\n",
    "# Taking the output\n",
    "desired_output = input_data.iloc[:,4:5]\n",
    "\n",
    "# Separating the input\n",
    "input_data = input_data.iloc[:, :4] \n",
    "\n",
    "#Converting to numpy array as matrix multiplication is easier in numpy\n",
    "input_data = np.array(input_data)\n",
    "desired_output = np.array(desired_output)\n",
    "\n",
    "#Adding bias input\n",
    "bias_input = np.ones(len(input_data))\n",
    "input_data = np.insert(input_data,0,bias_input,axis =1)\n",
    "\n",
    "momentum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neural_networks(object):\n",
    "    def __init__(self):\n",
    "        np.random.seed(100)\n",
    "        #Initializing\n",
    "        self.inputLayerSize = 4\n",
    "        self.outputLayerSize =1\n",
    "        self.hiddenLayerSize = 4\n",
    "        \n",
    "        \n",
    "        #Initializing the weights\n",
    "        self.W1 = np.random.uniform(-1,1,(self.inputLayerSize+1,self.hiddenLayerSize))\n",
    "        self.W2 = np.random.uniform(-1,1,(self.hiddenLayerSize,self.outputLayerSize))\n",
    "        self.bias =np.random.uniform(-1,1,self.outputLayerSize)\n",
    "        print \"Initialized W1 for above learning rate : \", self.W1\n",
    "        print \"Intialized W2 for above learning rate : \", self.W2\n",
    "\n",
    "#Code for forward feed\n",
    "    def forward_feed(self,X):\n",
    "        #z2 is the input to th hidden layer units\n",
    "        self.z2 = np.dot(X,self.W1)\n",
    "        \n",
    "        #a2 is the output of hidden layer units\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        \n",
    "        #z3 is the input to the output layer\n",
    "        self.z3 = np.dot(self.a2,self.W2)\n",
    "\n",
    "        #Output\n",
    "        self.yhat = self.sigmoid(self.z3)\n",
    "        return  self.yhat\n",
    "        \n",
    "    def sigmoid(self,z):\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derivative(self,z):\n",
    "        return (np.exp(-z)/((1+np.exp(-z))**2))\n",
    "    \n",
    "    def error_function(self,X,y):\n",
    "        self.yhat = self.forward_feed(X)\n",
    "        return np.abs((y - self.yhat))\n",
    "    \n",
    "    def gradient_function(self,X,y):\n",
    "        error = self.error_function(X,y)\n",
    "        if(sum(error<0.05)==len(X)):\n",
    "            print \"Final W1 : \", self.W1\n",
    "            print \"Final W2 : \", self.W2\n",
    "            return sum(error<0.05)\n",
    "        else:         \n",
    "            self.delta3 = np.multiply(-(y-self.yhat),self.sigmoid_derivative(self.z3))#+self.bias))\n",
    "            self.gradient_wrt_W2 = np.dot(self.a2.T,self.delta3)\n",
    "            delta2 = np.dot(self.delta3,self.W2.T)*self.sigmoid_derivative(self.z2)\n",
    "            self.gradient_wrt_W1 = np.dot(X.T,delta2)\n",
    "            self.update_weight()\n",
    "            return sum(error<0.05)\n",
    "        \n",
    "    def update_weight(self):\n",
    "        self.W1 = self.W1 - ((learning_rate)/(1-momentum)) * self.gradient_wrt_W1\n",
    "        self.W2 = self.W2 - ((learning_rate)/(1-momentum)) * self.gradient_wrt_W2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.05\n",
      "Initialized W1 for above learning rate :  [[ 0.08680988 -0.44326123 -0.15096482  0.68955226]\n",
      " [-0.99056229 -0.75686176  0.34149817  0.65170551]\n",
      " [-0.72658682  0.15018666  0.78264391 -0.58159576]\n",
      " [-0.62934356 -0.78324622 -0.56060501  0.95724757]\n",
      " [ 0.6233663  -0.65611797  0.6324495  -0.45185251]]\n",
      "Intialized W2 for above learning rate :  [[-0.13659163]\n",
      " [ 0.88005964]\n",
      " [ 0.63529876]\n",
      " [-0.3277761 ]]\n",
      "Final W1 :  [[ 3.08804523  3.3692949  -6.40732613  1.91982866]\n",
      " [-8.29458768 -2.08042579  1.88638647  7.47145221]\n",
      " [-7.62767624 -5.13205729  3.87435426 -5.64654901]\n",
      " [-8.28296229 -2.07988353  1.88619562  7.46893827]\n",
      " [ 8.27477797  2.07424457 -1.92300814 -7.56895356]]\n",
      "Final W2 :  [[-13.15691487]\n",
      " [ 16.60945528]\n",
      " [ 15.88700076]\n",
      " [ -9.42032172]]\n",
      "Yhat for learning_rate =  0.05 is : [[ 0.00887291]\n",
      " [ 0.966169  ]\n",
      " [ 0.97590732]\n",
      " [ 0.00989039]\n",
      " [ 0.96234416]\n",
      " [ 0.04393018]\n",
      " [ 0.03251345]\n",
      " [ 0.96127227]\n",
      " [ 0.97589177]\n",
      " [ 0.0099231 ]\n",
      " [ 0.04009434]\n",
      " [ 0.97538458]\n",
      " [ 0.03251233]\n",
      " [ 0.96127829]\n",
      " [ 0.95000089]\n",
      " [ 0.02920253]]\n",
      "Absolute error for learning_rate =  0.05 is:  [[ 0.00887291]\n",
      " [ 0.033831  ]\n",
      " [ 0.02409268]\n",
      " [ 0.00989039]\n",
      " [ 0.03765584]\n",
      " [ 0.04393018]\n",
      " [ 0.03251345]\n",
      " [ 0.03872773]\n",
      " [ 0.02410823]\n",
      " [ 0.0099231 ]\n",
      " [ 0.04009434]\n",
      " [ 0.02461542]\n",
      " [ 0.03251233]\n",
      " [ 0.03872171]\n",
      " [ 0.04999911]\n",
      " [ 0.02920253]]\n",
      "Learning rate:  0.1\n",
      "Initialized W1 for above learning rate :  [[ 0.08680988 -0.44326123 -0.15096482  0.68955226]\n",
      " [-0.99056229 -0.75686176  0.34149817  0.65170551]\n",
      " [-0.72658682  0.15018666  0.78264391 -0.58159576]\n",
      " [-0.62934356 -0.78324622 -0.56060501  0.95724757]\n",
      " [ 0.6233663  -0.65611797  0.6324495  -0.45185251]]\n",
      "Intialized W2 for above learning rate :  [[-0.13659163]\n",
      " [ 0.88005964]\n",
      " [ 0.63529876]\n",
      " [-0.3277761 ]]\n",
      "Final W1 :  [[ 3.09028967  3.36779809 -6.40713499  1.91997907]\n",
      " [-8.29676406 -2.08025372  1.88710261  7.46971678]\n",
      " [-7.62997123 -5.12979619  3.87303827 -5.64556967]\n",
      " [-8.28506526 -2.0797086   1.88691064  7.46719106]\n",
      " [ 8.27611975  2.07404363 -1.92388965 -7.56780893]]\n",
      "Final W2 :  [[-13.15995344]\n",
      " [ 16.61208291]\n",
      " [ 15.88316411]\n",
      " [ -9.41824906]]\n",
      "Yhat for learning_rate =  0.1 is : [[ 0.00886633]\n",
      " [ 0.96614995]\n",
      " [ 0.97591906]\n",
      " [ 0.00989319]\n",
      " [ 0.96235329]\n",
      " [ 0.04393189]\n",
      " [ 0.03250543]\n",
      " [ 0.96128355]\n",
      " [ 0.97590343]\n",
      " [ 0.00992609]\n",
      " [ 0.0401    ]\n",
      " [ 0.97539494]\n",
      " [ 0.03250431]\n",
      " [ 0.96128959]\n",
      " [ 0.95000092]\n",
      " [ 0.0291839 ]]\n",
      "Absolute error for learning_rate =  0.1 is:  [[ 0.00886633]\n",
      " [ 0.03385005]\n",
      " [ 0.02408094]\n",
      " [ 0.00989319]\n",
      " [ 0.03764671]\n",
      " [ 0.04393189]\n",
      " [ 0.03250543]\n",
      " [ 0.03871645]\n",
      " [ 0.02409657]\n",
      " [ 0.00992609]\n",
      " [ 0.0401    ]\n",
      " [ 0.02460506]\n",
      " [ 0.03250431]\n",
      " [ 0.03871041]\n",
      " [ 0.04999908]\n",
      " [ 0.0291839 ]]\n",
      "Learning rate:  0.15\n",
      "Initialized W1 for above learning rate :  [[ 0.08680988 -0.44326123 -0.15096482  0.68955226]\n",
      " [-0.99056229 -0.75686176  0.34149817  0.65170551]\n",
      " [-0.72658682  0.15018666  0.78264391 -0.58159576]\n",
      " [-0.62934356 -0.78324622 -0.56060501  0.95724757]\n",
      " [ 0.6233663  -0.65611797  0.6324495  -0.45185251]]\n",
      "Intialized W2 for above learning rate :  [[-0.13659163]\n",
      " [ 0.88005964]\n",
      " [ 0.63529876]\n",
      " [-0.3277761 ]]\n",
      "Final W1 :  [[ 3.0925964   3.36626991 -6.40693817  1.92014264]\n",
      " [-8.29900443 -2.08008039  1.88783475  7.46796842]\n",
      " [-7.63233346 -5.12748966  3.87169221 -5.6445885 ]\n",
      " [-8.28723077 -2.07953238  1.88764164  7.46543063]\n",
      " [ 8.27749985  2.07384028 -1.92479128 -7.56666858]]\n",
      "Final W2 :  [[-13.16309667]\n",
      " [ 16.61481762]\n",
      " [ 15.87927687]\n",
      " [ -9.41615062]]\n",
      "Yhat for learning_rate =  0.15 is : [[ 0.00885942]\n",
      " [ 0.96613085]\n",
      " [ 0.97593135]\n",
      " [ 0.00989591]\n",
      " [ 0.96236295]\n",
      " [ 0.04393317]\n",
      " [ 0.03249698]\n",
      " [ 0.96129543]\n",
      " [ 0.97591565]\n",
      " [ 0.00992899]\n",
      " [ 0.04010538]\n",
      " [ 0.97540579]\n",
      " [ 0.03249586]\n",
      " [ 0.9613015 ]\n",
      " [ 0.95000134]\n",
      " [ 0.02916461]]\n",
      "Absolute error for learning_rate =  0.15 is:  [[ 0.00885942]\n",
      " [ 0.03386915]\n",
      " [ 0.02406865]\n",
      " [ 0.00989591]\n",
      " [ 0.03763705]\n",
      " [ 0.04393317]\n",
      " [ 0.03249698]\n",
      " [ 0.03870457]\n",
      " [ 0.02408435]\n",
      " [ 0.00992899]\n",
      " [ 0.04010538]\n",
      " [ 0.02459421]\n",
      " [ 0.03249586]\n",
      " [ 0.0386985 ]\n",
      " [ 0.04999866]\n",
      " [ 0.02916461]]\n",
      "Learning rate:  0.2\n",
      "Initialized W1 for above learning rate :  [[ 0.08680988 -0.44326123 -0.15096482  0.68955226]\n",
      " [-0.99056229 -0.75686176  0.34149817  0.65170551]\n",
      " [-0.72658682  0.15018666  0.78264391 -0.58159576]\n",
      " [-0.62934356 -0.78324622 -0.56060501  0.95724757]\n",
      " [ 0.6233663  -0.65611797  0.6324495  -0.45185251]]\n",
      "Intialized W2 for above learning rate :  [[-0.13659163]\n",
      " [ 0.88005964]\n",
      " [ 0.63529876]\n",
      " [-0.3277761 ]]\n",
      "Final W1 :  [[ 3.0949704   3.36471516 -6.40671255  1.92031962]\n",
      " [-8.30130683 -2.07991042  1.88858019  7.46619779]\n",
      " [-7.63476729 -5.12514606  3.87029918 -5.64359779]\n",
      " [-8.28945662 -2.07935946  1.8883859   7.4636476 ]\n",
      " [ 8.27891425  2.07363882 -1.92571073 -7.565526  ]]\n",
      "Final W2 :  [[-13.16632462]\n",
      " [ 16.61762961]\n",
      " [ 15.87529875]\n",
      " [ -9.41400999]]\n",
      "Yhat for learning_rate =  0.2 is : [[ 0.00885222]\n",
      " [ 0.96611141]\n",
      " [ 0.97594409]\n",
      " [ 0.00989862]\n",
      " [ 0.96237282]\n",
      " [ 0.04393436]\n",
      " [ 0.03248847]\n",
      " [ 0.96130757]\n",
      " [ 0.97592831]\n",
      " [ 0.0099319 ]\n",
      " [ 0.04011071]\n",
      " [ 0.97541701]\n",
      " [ 0.03248734]\n",
      " [ 0.96131366]\n",
      " [ 0.95000152]\n",
      " [ 0.02914498]]\n",
      "Absolute error for learning_rate =  0.2 is:  [[ 0.00885222]\n",
      " [ 0.03388859]\n",
      " [ 0.02405591]\n",
      " [ 0.00989862]\n",
      " [ 0.03762718]\n",
      " [ 0.04393436]\n",
      " [ 0.03248847]\n",
      " [ 0.03869243]\n",
      " [ 0.02407169]\n",
      " [ 0.0099319 ]\n",
      " [ 0.04011071]\n",
      " [ 0.02458299]\n",
      " [ 0.03248734]\n",
      " [ 0.03868634]\n",
      " [ 0.04999848]\n",
      " [ 0.02914498]]\n",
      "Learning rate:  0.25\n",
      "Initialized W1 for above learning rate :  [[ 0.08680988 -0.44326123 -0.15096482  0.68955226]\n",
      " [-0.99056229 -0.75686176  0.34149817  0.65170551]\n",
      " [-0.72658682  0.15018666  0.78264391 -0.58159576]\n",
      " [-0.62934356 -0.78324622 -0.56060501  0.95724757]\n",
      " [ 0.6233663  -0.65611797  0.6324495  -0.45185251]]\n",
      "Intialized W2 for above learning rate :  [[-0.13659163]\n",
      " [ 0.88005964]\n",
      " [ 0.63529876]\n",
      " [-0.3277761 ]]\n",
      "Final W1 :  [[ 3.09741749  3.36308666 -6.40658242  1.92051604]\n",
      " [-8.30371198 -2.07971684  1.88936379  7.46444023]\n",
      " [-7.63728263 -5.12268902  3.86893353 -5.64263434]\n",
      " [-8.29178397 -2.07916292  1.88916831  7.46187737]\n",
      " [ 8.28040262  2.07341354 -1.92667334 -7.56441088]]\n",
      "Final W2 :  [[-13.16978956]\n",
      " [ 16.62071988]\n",
      " [ 15.8713906 ]\n",
      " [ -9.41188372]]\n",
      "Yhat for learning_rate =  0.25 is : [[ 0.00884441]\n",
      " [ 0.96609302]\n",
      " [ 0.97595799]\n",
      " [ 0.00990091]\n",
      " [ 0.9623848 ]\n",
      " [ 0.04393372]\n",
      " [ 0.03247778]\n",
      " [ 0.96132198]\n",
      " [ 0.97594212]\n",
      " [ 0.00993438]\n",
      " [ 0.04011472]\n",
      " [ 0.97542942]\n",
      " [ 0.03247664]\n",
      " [ 0.96132811]\n",
      " [ 0.95000485]\n",
      " [ 0.02912303]]\n",
      "Absolute error for learning_rate =  0.25 is:  [[ 0.00884441]\n",
      " [ 0.03390698]\n",
      " [ 0.02404201]\n",
      " [ 0.00990091]\n",
      " [ 0.0376152 ]\n",
      " [ 0.04393372]\n",
      " [ 0.03247778]\n",
      " [ 0.03867802]\n",
      " [ 0.02405788]\n",
      " [ 0.00993438]\n",
      " [ 0.04011472]\n",
      " [ 0.02457058]\n",
      " [ 0.03247664]\n",
      " [ 0.03867189]\n",
      " [ 0.04999515]\n",
      " [ 0.02912303]]\n",
      "Learning rate:  0.3\n",
      "Initialized W1 for above learning rate :  [[ 0.08680988 -0.44326123 -0.15096482  0.68955226]\n",
      " [-0.99056229 -0.75686176  0.34149817  0.65170551]\n",
      " [-0.72658682  0.15018666  0.78264391 -0.58159576]\n",
      " [-0.62934356 -0.78324622 -0.56060501  0.95724757]\n",
      " [ 0.6233663  -0.65611797  0.6324495  -0.45185251]]\n",
      "Intialized W2 for above learning rate :  [[-0.13659163]\n",
      " [ 0.88005964]\n",
      " [ 0.63529876]\n",
      " [-0.3277761 ]]\n",
      "Final W1 :  [[ 3.09994368  3.36144799 -6.40635417  1.92072599]\n",
      " [-8.30617035 -2.07954067  1.89015162  7.46263377]\n",
      " [-7.63987944 -5.12022292  3.8674718  -5.64163873]\n",
      " [-8.29416221 -2.07898371  1.88995491  7.4600578 ]\n",
      " [ 8.28191148  2.07320331 -1.92764561 -7.56327452]]\n",
      "Final W2 :  [[-13.17327501]\n",
      " [ 16.62379395]\n",
      " [ 15.8672755 ]\n",
      " [ -9.40966875]]\n",
      "Yhat for learning_rate =  0.3 is : [[ 0.00883644]\n",
      " [ 0.96607341]\n",
      " [ 0.97597204]\n",
      " [ 0.00990344]\n",
      " [ 0.96239598]\n",
      " [ 0.04393394]\n",
      " [ 0.03246814]\n",
      " [ 0.96133562]\n",
      " [ 0.97595609]\n",
      " [ 0.0099371 ]\n",
      " [ 0.04011943]\n",
      " [ 0.97544182]\n",
      " [ 0.03246701]\n",
      " [ 0.96134177]\n",
      " [ 0.95000604]\n",
      " [ 0.02910172]]\n",
      "Absolute error for learning_rate =  0.3 is:  [[ 0.00883644]\n",
      " [ 0.03392659]\n",
      " [ 0.02402796]\n",
      " [ 0.00990344]\n",
      " [ 0.03760402]\n",
      " [ 0.04393394]\n",
      " [ 0.03246814]\n",
      " [ 0.03866438]\n",
      " [ 0.02404391]\n",
      " [ 0.0099371 ]\n",
      " [ 0.04011943]\n",
      " [ 0.02455818]\n",
      " [ 0.03246701]\n",
      " [ 0.03865823]\n",
      " [ 0.04999396]\n",
      " [ 0.02910172]]\n",
      "Learning rate:  0.35\n",
      "Initialized W1 for above learning rate :  [[ 0.08680988 -0.44326123 -0.15096482  0.68955226]\n",
      " [-0.99056229 -0.75686176  0.34149817  0.65170551]\n",
      " [-0.72658682  0.15018666  0.78264391 -0.58159576]\n",
      " [-0.62934356 -0.78324622 -0.56060501  0.95724757]\n",
      " [ 0.6233663  -0.65611797  0.6324495  -0.45185251]]\n",
      "Intialized W2 for above learning rate :  [[-0.13659163]\n",
      " [ 0.88005964]\n",
      " [ 0.63529876]\n",
      " [-0.3277761 ]]\n",
      "Final W1 :  [[ 3.10255617  3.35979798 -6.40601691  1.92095083]\n",
      " [-8.30868567 -2.07938378  1.89094399  7.46077192]\n",
      " [-7.64256469 -5.11774665  3.86590353 -5.64060681]\n",
      " [-8.29659484 -2.07882366  1.890746    7.45818233]\n",
      " [ 8.28344175  2.07300976 -1.92862839 -7.56211328]]\n",
      "Final W2 :  [[-13.17677913]\n",
      " [ 16.62684458]\n",
      " [ 15.86292621]\n",
      " [ -9.4073526 ]]\n",
      "Yhat for learning_rate =  0.35 is : [[ 0.00882833]\n",
      " [ 0.96605238]\n",
      " [ 0.97598621]\n",
      " [ 0.00990626]\n",
      " [ 0.96240625]\n",
      " [ 0.04393513]\n",
      " [ 0.03245971]\n",
      " [ 0.96134835]\n",
      " [ 0.97597018]\n",
      " [ 0.00994012]\n",
      " [ 0.04012496]\n",
      " [ 0.97545421]\n",
      " [ 0.03245857]\n",
      " [ 0.96135452]\n",
      " [ 0.9500048 ]\n",
      " [ 0.02908116]]\n",
      "Absolute error for learning_rate =  0.35 is:  [[ 0.00882833]\n",
      " [ 0.03394762]\n",
      " [ 0.02401379]\n",
      " [ 0.00990626]\n",
      " [ 0.03759375]\n",
      " [ 0.04393513]\n",
      " [ 0.03245971]\n",
      " [ 0.03865165]\n",
      " [ 0.02402982]\n",
      " [ 0.00994012]\n",
      " [ 0.04012496]\n",
      " [ 0.02454579]\n",
      " [ 0.03245857]\n",
      " [ 0.03864548]\n",
      " [ 0.0499952 ]\n",
      " [ 0.02908116]]\n",
      "Learning rate:  0.4\n",
      "Initialized W1 for above learning rate :  [[ 0.08680988 -0.44326123 -0.15096482  0.68955226]\n",
      " [-0.99056229 -0.75686176  0.34149817  0.65170551]\n",
      " [-0.72658682  0.15018666  0.78264391 -0.58159576]\n",
      " [-0.62934356 -0.78324622 -0.56060501  0.95724757]\n",
      " [ 0.6233663  -0.65611797  0.6324495  -0.45185251]]\n",
      "Intialized W2 for above learning rate :  [[-0.13659163]\n",
      " [ 0.88005964]\n",
      " [ 0.63529876]\n",
      " [-0.3277761 ]]\n",
      "Final W1 :  [[ 3.10526315  3.35813901 -6.40554762  1.92119176]\n",
      " [-8.31125919 -2.07925054  1.89173928  7.45884403]\n",
      " [-7.645346   -5.11526488  3.86421015 -5.6395307 ]\n",
      " [-8.29908278 -2.07868716  1.89153995  7.45624027]\n",
      " [ 8.28499138  2.07283692 -1.92962073 -7.56092039]]\n",
      "Final W2 :  [[-13.1802876 ]\n",
      " [ 16.62984701]\n",
      " [ 15.85829684]\n",
      " [ -9.40491558]]\n",
      "Yhat for learning_rate =  0.4 is : [[ 0.0088201 ]\n",
      " [ 0.96602962]\n",
      " [ 0.97600046]\n",
      " [ 0.00990945]\n",
      " [ 0.96241528]\n",
      " [ 0.04393763]\n",
      " [ 0.03245285]\n",
      " [ 0.96135984]\n",
      " [ 0.97598434]\n",
      " [ 0.00994352]\n",
      " [ 0.04013157]\n",
      " [ 0.97546647]\n",
      " [ 0.0324517 ]\n",
      " [ 0.96136605]\n",
      " [ 0.9500005 ]\n",
      " [ 0.02906162]]\n",
      "Absolute error for learning_rate =  0.4 is:  [[ 0.0088201 ]\n",
      " [ 0.03397038]\n",
      " [ 0.02399954]\n",
      " [ 0.00990945]\n",
      " [ 0.03758472]\n",
      " [ 0.04393763]\n",
      " [ 0.03245285]\n",
      " [ 0.03864016]\n",
      " [ 0.02401566]\n",
      " [ 0.00994352]\n",
      " [ 0.04013157]\n",
      " [ 0.02453353]\n",
      " [ 0.0324517 ]\n",
      " [ 0.03863395]\n",
      " [ 0.0499995 ]\n",
      " [ 0.02906162]]\n",
      "Learning rate:  0.45\n",
      "Initialized W1 for above learning rate :  [[ 0.08680988 -0.44326123 -0.15096482  0.68955226]\n",
      " [-0.99056229 -0.75686176  0.34149817  0.65170551]\n",
      " [-0.72658682  0.15018666  0.78264391 -0.58159576]\n",
      " [-0.62934356 -0.78324622 -0.56060501  0.95724757]\n",
      " [ 0.6233663  -0.65611797  0.6324495  -0.45185251]]\n",
      "Intialized W2 for above learning rate :  [[-0.13659163]\n",
      " [ 0.88005964]\n",
      " [ 0.63529876]\n",
      " [-0.3277761 ]]\n",
      "Final W1 :  [[ 3.10807469  3.35626092 -6.40553322  1.92147272]\n",
      " [-8.31406484 -2.07901474  1.89264933  7.45702345]\n",
      " [-7.64825239 -5.11243607  3.86275158 -5.6385853 ]\n",
      " [-8.30180257 -2.0784482   1.89244867  7.45440538]\n",
      " [ 8.28673651  2.07256482 -1.93073549 -7.55983693]]\n",
      "Final W2 :  [[-13.18448656]\n",
      " [ 16.63371954]\n",
      " [ 15.85417051]\n",
      " [ -9.40263822]]\n",
      "Yhat for learning_rate =  0.45 is : [[ 0.00881026]\n",
      " [ 0.96601184]\n",
      " [ 0.97601792]\n",
      " [ 0.00991101]\n",
      " [ 0.96243195]\n",
      " [ 0.04393342]\n",
      " [ 0.03243769]\n",
      " [ 0.96137947]\n",
      " [ 0.97600172]\n",
      " [ 0.00994528]\n",
      " [ 0.04013322]\n",
      " [ 0.97548226]\n",
      " [ 0.03243655]\n",
      " [ 0.9613857 ]\n",
      " [ 0.95000918]\n",
      " [ 0.02903396]]\n",
      "Absolute error for learning_rate =  0.45 is:  [[ 0.00881026]\n",
      " [ 0.03398816]\n",
      " [ 0.02398208]\n",
      " [ 0.00991101]\n",
      " [ 0.03756805]\n",
      " [ 0.04393342]\n",
      " [ 0.03243769]\n",
      " [ 0.03862053]\n",
      " [ 0.02399828]\n",
      " [ 0.00994528]\n",
      " [ 0.04013322]\n",
      " [ 0.02451774]\n",
      " [ 0.03243655]\n",
      " [ 0.0386143 ]\n",
      " [ 0.04999082]\n",
      " [ 0.02903396]]\n",
      "Learning rate:  0.5\n",
      "Initialized W1 for above learning rate :  [[ 0.08680988 -0.44326123 -0.15096482  0.68955226]\n",
      " [-0.99056229 -0.75686176  0.34149817  0.65170551]\n",
      " [-0.72658682  0.15018666  0.78264391 -0.58159576]\n",
      " [-0.62934356 -0.78324622 -0.56060501  0.95724757]\n",
      " [ 0.6233663  -0.65611797  0.6324495  -0.45185251]]\n",
      "Intialized W2 for above learning rate :  [[-0.13659163]\n",
      " [ 0.88005964]\n",
      " [ 0.63529876]\n",
      " [-0.3277761 ]]\n",
      "Final W1 :  [[ 3.11100077  3.35441877 -6.40521857  1.92176841]\n",
      " [-8.3168993  -2.07883726  1.89353768  7.45507611]\n",
      " [-7.65126983 -5.10967724  3.86105289 -5.63754224]\n",
      " [-8.30454697 -2.07826737  1.89333561  7.45244293]\n",
      " [ 8.28846277  2.07234602 -1.93183675 -7.5586772 ]]\n",
      "Final W2 :  [[-13.18852272]\n",
      " [ 16.63730635]\n",
      " [ 15.84949558]\n",
      " [ -9.40013441]]\n",
      "Yhat for learning_rate =  0.5 is : [[ 0.00880067]\n",
      " [ 0.96599021]\n",
      " [ 0.97603466]\n",
      " [ 0.00991354]\n",
      " [ 0.96244493]\n",
      " [ 0.04393282]\n",
      " [ 0.03242686]\n",
      " [ 0.96139528]\n",
      " [ 0.97601837]\n",
      " [ 0.00994802]\n",
      " [ 0.04013774]\n",
      " [ 0.97549699]\n",
      " [ 0.03242571]\n",
      " [ 0.96140154]\n",
      " [ 0.95001018]\n",
      " [ 0.02900977]]\n",
      "Absolute error for learning_rate =  0.5 is:  [[ 0.00880067]\n",
      " [ 0.03400979]\n",
      " [ 0.02396534]\n",
      " [ 0.00991354]\n",
      " [ 0.03755507]\n",
      " [ 0.04393282]\n",
      " [ 0.03242686]\n",
      " [ 0.03860472]\n",
      " [ 0.02398163]\n",
      " [ 0.00994802]\n",
      " [ 0.04013774]\n",
      " [ 0.02450301]\n",
      " [ 0.03242571]\n",
      " [ 0.03859846]\n",
      " [ 0.04998982]\n",
      " [ 0.02900977]]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "No_of_epochs={}\n",
    "Final_output = {}\n",
    "\n",
    "while(learning_rate<=0.5):\n",
    "    print \"Learning rate: \",learning_rate\n",
    "    neural_network = Neural_networks()\n",
    "    i=0\n",
    "    Error_count = 0\n",
    "    while(Error_count!=len(input_data)):\n",
    "        i = i+1\n",
    "#        print \"Epoch: \",i\n",
    "        Error_count = neural_network.gradient_function(input_data,desired_output)\n",
    "        if(i>500000):\n",
    "            break\n",
    "    No_of_epochs[learning_rate]=i\n",
    "    Final_output[learning_rate] = neural_network.yhat\n",
    "    print \"Yhat for learning_rate = \",learning_rate,\"is :\",neural_network.yhat\n",
    "    print \"Absolute error for learning_rate = \",learning_rate,\"is: \",np.abs(neural_network.yhat-desired_output)\n",
    "    learning_rate+=0.05\n",
    "    Error_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
